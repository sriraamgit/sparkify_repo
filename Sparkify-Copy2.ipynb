{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark_dist_explore in /opt/conda/lib/python3.6/site-packages (0.1.8)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (1.12.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (0.19.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (2.1.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (0.23.3)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (1.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (2.6.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (2017.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib->pyspark_dist_explore) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark_dist_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries packages needed for the project\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, BooleanType, StructType, StructField\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import avg, max, count, when, desc, asc, udf, col, isnan, sort_array, format_string\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier, NaiveBayes, LogisticRegression\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer,VectorAssembler,OneHotEncoder\n",
    "from pyspark_dist_explore import hist\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SQLContext\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Sparkify Project') \\\n",
    "    .getOrCreate()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the dataset as asked above\n",
    "sparkify_event_data = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(sparkify_event_data)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean the data from NAN values\n",
      "Exploratory data analysis of gender data\n",
      "+------+----------+\n",
      "|userId|check_male|\n",
      "+------+----------+\n",
      "|    10|         1|\n",
      "|   100|         1|\n",
      "|100001|         0|\n",
      "|100002|         0|\n",
      "|100003|         0|\n",
      "+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Exploratory data analysis of pages data...\n",
      "+------+---------------+----------+----+---------+----------+------+------------+-----+--------+-------------+---------+--------+--------+----+-------+-----+-------------+\n",
      "|userId|SubmitDowngrade|ThumbsDown|Home|Downgrade|RollAdvert|Logout|SaveSettings|About|Settings|AddtoPlaylist|AddFriend|NextSong|ThumbsUp|Help|Upgrade|Error|SubmitUpgrade|\n",
      "+------+---------------+----------+----+---------+----------+------+------------+-----+--------+-------------+---------+--------+--------+----+-------+-----+-------------+\n",
      "|    10|              0|         4|  30|        7|         1|    11|           1|    2|       7|            9|       12|     673|      37|   1|      0|    0|            0|\n",
      "|   100|              1|        27| 105|       30|        25|    35|           5|   12|      11|           61|       49|    2682|     148|  18|      1|    3|            1|\n",
      "|100001|              0|         2|  11|        0|        14|     7|           0|    0|       1|            3|        2|     133|       8|   1|      2|    1|            0|\n",
      "|100002|              0|         0|   6|        2|         3|     1|           0|    0|       0|            5|        1|     195|       5|   0|      0|    0|            0|\n",
      "|100003|              0|         0|   7|        0|         9|     3|           0|    0|       0|            2|        0|      51|       3|   1|      0|    0|            0|\n",
      "+------+---------------+----------+----+---------+----------+------+------------+-----+--------+-------------+---------+--------+--------+----+-------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Exploratory data analysis of songs count for session...\n",
      "+------+-------+-----------------+----------+\n",
      "|userId|songs_c|songs_session_a_v|sessions_c|\n",
      "+------+-------+-----------------+----------+\n",
      "|    10|    813|            135.5|         6|\n",
      "|   100|   3263|93.22857142857143|        35|\n",
      "|100001|    197|            49.25|         4|\n",
      "|100002|    216|             54.0|         4|\n",
      "|100003|     82|             41.0|         2|\n",
      "+------+-------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Exploratory data analysis of lifetime of user from registration timestamp...\n",
      "+------+------------------+\n",
      "|userId|  lifetime_in_days|\n",
      "+------+------------------+\n",
      "|    10| 51.76265046296296|\n",
      "|   100| 64.87377314814815|\n",
      "|100001| 44.80021990740741|\n",
      "|100002|160.47207175925925|\n",
      "|100003|22.748113425925926|\n",
      "+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Exploratory data analysis of distinct status as columns...\n",
      "+------+----------+----------+----------+\n",
      "|userId|status_307|status_404|status_200|\n",
      "+------+----------+----------+----------+\n",
      "|    10|        65|         0|       730|\n",
      "|   100|       266|         3|      2945|\n",
      "|100001|        20|         1|       166|\n",
      "|100002|         7|         0|       211|\n",
      "|100003|         7|         0|        71|\n",
      "+------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Exploratory data analysis of level of user, paid user gets 1...\n",
      "+------+------------+\n",
      "|userId|paid_success|\n",
      "+------+------------+\n",
      "|    10|           1|\n",
      "|   100|           1|\n",
      "|100001|           0|\n",
      "|100002|           1|\n",
      "|100003|           0|\n",
      "+------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Exploratory data analysis of label of user, for cancellation conformation level is 1...\n",
      "+------+-----+\n",
      "|userId|label|\n",
      "+------+-----+\n",
      "|    10|    0|\n",
      "|   100|    0|\n",
      "+------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "['check_male', 'paid_success', 'lifetime_in_days', 'SubmitDowngrade', 'ThumbsDown', 'Home', 'Downgrade', 'RollAdvert', 'Logout', 'SaveSettings', 'About', 'Settings', 'AddtoPlaylist', 'AddFriend', 'NextSong', 'ThumbsUp', 'Help', 'Upgrade', 'Error', 'SubmitUpgrade', 'songs_c', 'songs_session_a_v', 'sessions_c', 'status_307', 'status_404', 'status_200']\n"
     ]
    }
   ],
   "source": [
    "def new_df(df):\n",
    "    print('Clean the data from NAN values')\n",
    "    df_cleaned = df.dropna(how='any', subset=['gender'])\n",
    "    print('Exploratory data analysis of gender data')\n",
    "    gender_df = df_cleaned.select('userId', 'gender').distinct()\\\n",
    "                     .withColumn('check_male', (df_cleaned.gender == 'M').cast(IntegerType()))\\\n",
    "                     .sort('userId')\\\n",
    "                     .select('userId', 'check_male')                  \n",
    "    gender_df.show(5)\n",
    "   \n",
    "    \n",
    "    print('Exploratory data analysis of pages data...')\n",
    "    pages = [row[0] for row in df_cleaned.select('page').distinct().collect()]\n",
    "    pages.remove('Cancel')\n",
    "    pages.remove('Cancellation Confirmation')\n",
    "    windowval = Window.partitionBy('userId')\n",
    "    page_df = df_cleaned\n",
    "    for page in pages:\n",
    "        #replace any spaces in page names to without spaces\n",
    "        page_df = page_df.withColumn(page.replace(' ', ''), Fsum((df_cleaned.page == page).cast(IntegerType())).over(windowval))\n",
    "    other_pages = []\n",
    "    for page in pages:\n",
    "        other_pages.append(page.replace(' ', ''))\n",
    "    page_df = page_df.select('userId', *other_pages).dropDuplicates().sort('userId')\n",
    "    page_df.show(5)\n",
    "    \n",
    "    print('Exploratory data analysis of songs count for session...')\n",
    "    songs_df = df_cleaned.select('userId','sessionId','itemInSession') \\\n",
    "                            .groupby('userId','sessionId') \\\n",
    "                            .agg({'itemInSession':'max'}) \\\n",
    "                            .groupby('userId') \\\n",
    "                            .agg( Fsum('max(itemInSession)'), avg('max(itemInSession)'), count('sessionId') ) \\\n",
    "                            .withColumnRenamed('avg(max(itemInSession))','songs_session_a_v') \\\n",
    "                            .withColumnRenamed('sum(max(itemInSession))', 'songs_c') \\\n",
    "                            .withColumnRenamed('count(sessionId)', 'sessions_c') \\\n",
    "                            .sort('userId')\n",
    "    songs_df.show(5)\n",
    "    \n",
    "    print('Exploratory data analysis of lifetime of user from registration timestamp...')\n",
    "    lifetime_df = df_cleaned.groupby('userId') \\\n",
    "                  .agg({'registration':'min', 'ts':'max'})\\\n",
    "                  .withColumn('lifetime_in_days', (col('max(ts)')-col('min(registration)'))/1000/60/60/24)\\\n",
    "                  .select('userId', 'lifetime_in_days') \\\n",
    "                  .sort('userId')\n",
    "    lifetime_df.show(5)\n",
    "    \n",
    "    print('Exploratory data analysis of distinct status as columns...')\n",
    "    st_list = [str(row[0]) for row in df_cleaned.select('status').distinct().collect()]\n",
    "    st_df = df_cleaned\n",
    "    for status in st_list:\n",
    "        st_df = st_df.withColumn('status_'+status, Fsum((df_cleaned.status == status).cast(IntegerType())).over(windowval))\n",
    "    st_df = st_df.select('userId', 'status_307', 'status_404', 'status_200').distinct().sort('userId')\n",
    "    st_df.show(5)\n",
    "    \n",
    "    print('Exploratory data analysis of level of user, paid user gets 1...')\n",
    "    lev_df = df_cleaned.select('userId', 'level').distinct() \\\n",
    "                      .withColumn('u_lev_paid', (df_cleaned.level == 'paid').cast(IntegerType())) \\\n",
    "                      .groupby('userId') \\\n",
    "                      .agg({'u_lev_paid':'max'}) \\\n",
    "                      .select('userId', 'max(u_lev_paid)') \\\n",
    "                      .withColumnRenamed('max(u_lev_paid)', 'paid_success') \\\n",
    "                      .sort('userId')\n",
    "    lev_df.show(5)\n",
    "    \n",
    "    print('Exploratory data analysis of label of user, for cancellation conformation level is 1...')\n",
    "    phase_ev = udf(lambda x: 1 if x=='Cancellation Confirmation' else 0, IntegerType())\n",
    "    lab_df = df_cleaned.withColumn('phase', phase_ev(df_cleaned['page']))\n",
    "    lab_df = lab_df.withColumn('label', Fsum('phase').over(windowval))\n",
    "    lab_df = lab_df.select('userId', 'label').dropDuplicates().sort('userId')\n",
    "    lab_df.show(2)\n",
    "    \n",
    "   \n",
    "    features_df = lab_df.select('userId', 'label').dropDuplicates().join(gender_df, 'userId').join(lev_df, 'userId') \\\n",
    "        .join(lifetime_df, 'userId').join(page_df, 'userId').join(songs_df, 'userId').join(st_df, 'userId').dropDuplicates() \\\n",
    "        .sort('userId')\n",
    "    features = list(features_df.columns)\n",
    "    features.remove('userId')\n",
    "    features.remove('label')\n",
    "    print(features)\n",
    "    return(features_df, features)\n",
    "features_df, features = new_df(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_df, features = new_df(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask\n",
    "from flask import render_template, request, jsonify\n",
    "app = Flask(__name__)\n",
    "@app.route(\"/sparkify_pr\")\n",
    "def upload():\n",
    "    return render_template(\"sparkify_pr.html\")\n",
    "    \n",
    "@app.route('/new_df', methods=['POST'])\n",
    "def new_df(df):\n",
    "    print('Clean the data from NAN values')\n",
    "    df_cleaned = df.dropna(how='any', subset=['gender'])\n",
    "    print('Exploratory data analysis of gender data')\n",
    "    gender_df = df_cleaned.select('userId', 'gender').distinct()\\\n",
    "                     .withColumn('check_male', (df_cleaned.gender == 'M').cast(IntegerType()))\\\n",
    "                     .sort('userId')\\\n",
    "                     .select('userId', 'check_male')                  \n",
    "    gender_df.show(5)\n",
    "   \n",
    "    \n",
    "    print('Exploratory data analysis of pages data...')\n",
    "    pages = [row[0] for row in df_cleaned.select('page').distinct().collect()]\n",
    "    pages.remove('Cancel')\n",
    "    pages.remove('Cancellation Confirmation')\n",
    "    windowval = Window.partitionBy('userId')\n",
    "    page_df = df_cleaned\n",
    "    for page in pages:\n",
    "        #replace any spaces in page names to without spaces\n",
    "        page_df = page_df.withColumn(page.replace(' ', ''), Fsum((df_cleaned.page == page).cast(IntegerType())).over(windowval))\n",
    "    other_pages = []\n",
    "    for page in pages:\n",
    "        other_pages.append(page.replace(' ', ''))\n",
    "    page_df = page_df.select('userId', *other_pages).dropDuplicates().sort('userId')\n",
    "    page_df.show(5)\n",
    "    \n",
    "    print('Exploratory data analysis of songs count for session...')\n",
    "    songs_df = df_cleaned.select('userId','sessionId','itemInSession') \\\n",
    "                            .groupby('userId','sessionId') \\\n",
    "                            .agg({'itemInSession':'max'}) \\\n",
    "                            .groupby('userId') \\\n",
    "                            .agg( Fsum('max(itemInSession)'), avg('max(itemInSession)'), count('sessionId') ) \\\n",
    "                            .withColumnRenamed('avg(max(itemInSession))','songs_session_a_v') \\\n",
    "                            .withColumnRenamed('sum(max(itemInSession))', 'songs_c') \\\n",
    "                            .withColumnRenamed('count(sessionId)', 'sessions_c') \\\n",
    "                            .sort('userId')\n",
    "    songs_df.show(5)\n",
    "    \n",
    "    print('Exploratory data analysis of lifetime of user from registration timestamp...')\n",
    "    lifetime_df = df_cleaned.groupby('userId') \\\n",
    "                  .agg({'registration':'min', 'ts':'max'})\\\n",
    "                  .withColumn('lifetime_in_days', (col('max(ts)')-col('min(registration)'))/1000/60/60/24)\\\n",
    "                  .select('userId', 'lifetime_in_days') \\\n",
    "                  .sort('userId')\n",
    "    lifetime_df.show(5)\n",
    "    \n",
    "    print('Exploratory data analysis of distinct status as columns...')\n",
    "    st_list = [str(row[0]) for row in df_cleaned.select('status').distinct().collect()]\n",
    "    st_df = df_cleaned\n",
    "    for status in st_list:\n",
    "        st_df = st_df.withColumn('status_'+status, Fsum((df_cleaned.status == status).cast(IntegerType())).over(windowval))\n",
    "    st_df = st_df.select('userId', 'status_307', 'status_404', 'status_200').distinct().sort('userId')\n",
    "    st_df.show(5)\n",
    "    \n",
    "    print('Exploratory data analysis of level of user, paid user gets 1...')\n",
    "    lev_df = df_cleaned.select('userId', 'level').distinct() \\\n",
    "                      .withColumn('u_lev_paid', (df_cleaned.level == 'paid').cast(IntegerType())) \\\n",
    "                      .groupby('userId') \\\n",
    "                      .agg({'u_lev_paid':'max'}) \\\n",
    "                      .select('userId', 'max(u_lev_paid)') \\\n",
    "                      .withColumnRenamed('max(u_lev_paid)', 'paid_success') \\\n",
    "                      .sort('userId')\n",
    "    lev_df.show(5)\n",
    "    \n",
    "    print('Exploratory data analysis of label of user, for cancellation conformation level is 1...')\n",
    "    phase_ev = udf(lambda x: 1 if x=='Cancellation Confirmation' else 0, IntegerType())\n",
    "    lab_df = df_cleaned.withColumn('phase', phase_ev(df_cleaned['page']))\n",
    "    lab_df = lab_df.withColumn('label', Fsum('phase').over(windowval))\n",
    "    lab_df = lab_df.select('userId', 'label').dropDuplicates().sort('userId')\n",
    "    lab_df.show(2)\n",
    "    \n",
    "   \n",
    "    features_df = lab_df.select('userId', 'label').dropDuplicates().join(gender_df, 'userId').join(lev_df, 'userId') \\\n",
    "        .join(lifetime_df, 'userId').join(page_df, 'userId').join(songs_df, 'userId').join(st_df, 'userId').dropDuplicates() \\\n",
    "        .sort('userId')\n",
    "    features = list(features_df.columns)\n",
    "    features.remove('userId')\n",
    "    features.remove('label')\n",
    "    print(features)\n",
    "    return(features_df, features)\n",
    "    features_df, features = new_df(df=df)\n",
    "#if __name__ == \"__main__\":\n",
    "    #app.run(port=4010, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "    #app.run(port=4018, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
